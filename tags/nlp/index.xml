<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>NLP on Alex Day</title><link>https://www.alexday.me/tags/nlp/</link><description>Recent content in NLP on Alex Day</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>&lt;a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC BY-NC 4.0&lt;/a></copyright><lastBuildDate>Wed, 06 Nov 2019 00:00:00 +0000</lastBuildDate><atom:link href="https://www.alexday.me/tags/nlp/index.xml" rel="self" type="application/rss+xml"/><item><title>Dependency Tree Collapse for N-Gram Generation</title><link>https://www.alexday.me/posts/dependency-tree-collapse/</link><pubDate>Wed, 06 Nov 2019 00:00:00 +0000</pubDate><guid>https://www.alexday.me/posts/dependency-tree-collapse/</guid><description>Introduction Throughout the past semester I have been working on my senior capstone project for my CS undergraduate. The project is to create Emoji summaries for sentences and one of the integral parts of this algorithm is separating a sentence into a sequence of n-grams that represent it. In the initial algorithm, I took a naive approach of generating every single combination of n-grams, summarizing them all, and then returning the summary with the highest result.</description></item><item><title>Naive Sentence to Emoji Translation</title><link>https://www.alexday.me/posts/naive-emoji-summarization/</link><pubDate>Mon, 07 Oct 2019 00:00:00 +0000</pubDate><guid>https://www.alexday.me/posts/naive-emoji-summarization/</guid><description>Motivation My senior capstone project for my computer science degree is research focused on summarizing sentences. My group mate and I decided to try and accomplish this by converting sentences into Emoji. We think that this will produce a more information-dense string. This problem is rather similar to a plethora of different problems in computer science and other, unrelated, domains. Within computer science, it is adjacent to the Emoji prediction and Emoji embedding problems.</description></item><item><title>TF-IDF and Document Summarization</title><link>https://www.alexday.me/posts/tf-idf/</link><pubDate>Thu, 25 Apr 2019 00:00:00 +0000</pubDate><guid>https://www.alexday.me/posts/tf-idf/</guid><description>Term Frequency-Inverse Document Frequency (commonly abbreviated as TF-IDF) is a formula commonly used in Natural Language Processing (NLP) to determine the relative importance of a word. The formula is comprised of two sub-formulas, term frequency and inverse document frequency. The basic assumption of this formula is that if a word appears more in one document and less in every other document in the corpus then it is very important to that specific document.</description></item></channel></rss>