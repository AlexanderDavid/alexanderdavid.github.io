<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>NLP on Alex Day</title><link>https://www.alexday.me/tags/nlp/</link><description>Recent content in NLP on Alex Day</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>&lt;a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC BY-NC 4.0&lt;/a></copyright><lastBuildDate>Wed, 20 May 2020 23:41:25 -0400</lastBuildDate><atom:link href="https://www.alexday.me/tags/nlp/index.xml" rel="self" type="application/rss+xml"/><item><title>Stock Clustering</title><link>https://www.alexday.me/posts/stock-clustering/</link><pubDate>Wed, 20 May 2020 23:41:25 -0400</pubDate><guid>https://www.alexday.me/posts/stock-clustering/</guid><description>One of the problems in Natural Language Processing (and a problem I&amp;#39;m facing at work) is how to cluster documents into groups based on their contents. There are two broad approaches to solving the document clustering problem, supervised and unsupervised machine learning. Supervised machine learning relies on labeled data and unsupervised learning tries to categorize the data without any prior labels. These two methods both have their ups and downs that I will not go into here.</description></item><item><title>Emoji Summarization</title><link>https://www.alexday.me/projects/emoji/</link><pubDate>Fri, 13 Dec 2019 00:00:00 +0000</pubDate><guid>https://www.alexday.me/projects/emoji/</guid><description>For my senior year capstone project at Clarion University I worked with another student to try and summarize sentences by producing a string of emojis. We accomplished this goal through the use of vector embeddings. The initial algorithm is explained in a blog posts, as well as a novel sentence segmentation algorithm that I developed as a consequence of the project. We presented this project at the Penn York conference(slides, abstract) in Clarion and published a paper at the 2020 PACISE conference.</description></item><item><title>Dependency Tree Collapse for N-Gram Generation</title><link>https://www.alexday.me/posts/dependency-tree-collapse/</link><pubDate>Wed, 06 Nov 2019 00:00:00 +0000</pubDate><guid>https://www.alexday.me/posts/dependency-tree-collapse/</guid><description>Introduction Throughout the past semester I have been working on my senior capstone project for my CS undergraduate. The project is to create Emoji summaries for sentences and one of the integral parts of this algorithm is separating a sentence into a sequence of n-grams that represent it. In the initial algorithm, I took a naive approach of generating every single combination of n-grams, summarizing them all, and then returning the summary with the highest result.</description></item><item><title>Naive Sentence to Emoji Translation</title><link>https://www.alexday.me/posts/naive-emoji-summarization/</link><pubDate>Mon, 07 Oct 2019 00:00:00 +0000</pubDate><guid>https://www.alexday.me/posts/naive-emoji-summarization/</guid><description>Motivation My senior capstone project for my computer science degree is research focused on summarizing sentences. My group mate and I decided to try and accomplish this by converting sentences into Emoji. We think that this will produce a more information-dense string. This problem is rather similar to a plethora of different problems in computer science and other, unrelated, domains. Within computer science, it is adjacent to the Emoji prediction and Emoji embedding problems.</description></item><item><title>TF-IDF and Document Summarization</title><link>https://www.alexday.me/posts/tf-idf/</link><pubDate>Thu, 25 Apr 2019 00:00:00 +0000</pubDate><guid>https://www.alexday.me/posts/tf-idf/</guid><description>Term Frequency-Inverse Document Frequency (commonly abbreviated as TF-IDF) is a formula commonly used in Natural Language Processing (NLP) to determine the relative importance of a word. The formula is comprised of two sub-formulas, term frequency and inverse document frequency. The basic assumption of this formula is that if a word appears more in one document and less in every other document in the corpus then it is very important to that specific document.</description></item></channel></rss>