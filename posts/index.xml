<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Alex Day</title><link>/posts/</link><description>Recent content in Posts on Alex Day</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Â© 2020 Alex Day</copyright><lastBuildDate>Sun, 12 Apr 2020 15:35:54 -0400</lastBuildDate><atom:link href="/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Emacs for Email</title><link>/posts/emacs-for-email/</link><pubDate>Sun, 12 Apr 2020 15:35:54 -0400</pubDate><guid>/posts/emacs-for-email/</guid><description>Email management, when heavily abstracted, is simple. To start reading email offline on your own PC you need three programs: Sync email to/from IMAP server (OfflineIMAP) Manage email on your PC (mu and mu4e) Send email (msmtp) Once these three parts are working together then email can be downloaded, viewed, and replied to. Getting these programs working is no easy task, however.</description></item><item><title>Dependency Tree Collapse for N-Gram Generation</title><link>/posts/dependency-tree-collapse/</link><pubDate>Wed, 06 Nov 2019 00:00:00 +0000</pubDate><guid>/posts/dependency-tree-collapse/</guid><description>Introduction Throughout the past semester I have been working on my senior capstone project for my CS undergraduate. The project is to create Emoji summaries for sentences and one of the integral parts of this algorithm is separating a sentence into a sequence of n-grams that represent it. In the initial algorithm, I took a naive approach of generating every single combination of n-grams, summarizing them all, and then returning the summary with the highest result.</description></item><item><title>Naive Sentence to Emoji Translation</title><link>/posts/naive-emoji-summarization/</link><pubDate>Mon, 07 Oct 2019 00:00:00 +0000</pubDate><guid>/posts/naive-emoji-summarization/</guid><description>Motivation My senior capstone project for my computer science degree is research focused on summarizing sentences. My group mate and I decided to try and accomplish this by converting sentences into Emoji. We think that this will produce a more information-dense string. This problem is rather similar to a plethora of different problems in computer science and other, unrelated, domains. Within computer science, it is adjacent to the Emoji prediction and Emoji embedding problems.</description></item><item><title>TF-IDF and Document Summarization</title><link>/posts/tf-idf/</link><pubDate>Thu, 25 Apr 2019 00:00:00 +0000</pubDate><guid>/posts/tf-idf/</guid><description>Term Frequency-Inverse Document Frequency (commonly abbreviated as TF-IDF) is a formula commonly used in Natural Language Processing (NLP) to determine the relative importance of a word. The formula is comprised of two sub-formulas, term frequency and inverse document frequency. The basic assumption of this formula is that if a word appears more in one document and less in every other document in the corpus then it is very important to that specific document.</description></item></channel></rss>