<!doctype html><html lang=en><head><title>TF-IDF and Document Summarization :: Alex Day</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=description content="Term Frequency-Inverse Document Frequency (commonly abbreviated as TF-IDF) is a formula commonly used in Natural Language Processing (NLP) to determine the relative importance of a word. The formula is comprised of two sub-formulas, term frequency and inverse document frequency. The basic assumption of this formula is that if a word appears more in one document and less in every other document in the corpus then it is very important to that specific document."><meta name=keywords content="Coding,PhD,Clemson"><meta name=robots content="noodp"><link rel=canonical href=/posts/tf-idf/><link rel=stylesheet href=/assets/style.css><link rel=stylesheet href=/assets/green.css><link rel=stylesheet href=/style.css><link rel=apple-touch-icon-precomposed sizes=144x144 href=/img/apple-touch-icon-144-precomposed.png><link rel="shortcut icon" href=/favicon.ico><meta name=twitter:card content="summary"><meta name=twitter:site content><meta name=twitter:creator content><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="og:title" content="TF-IDF and Document Summarization :: Alex Day"><meta property="og:description" content="Using TF-IDF to extractively summarize a document within a corpus"><meta property="og:url" content="/posts/tf-idf/"><meta property="og:site_name" content="TF-IDF and Document Summarization"><meta property="og:image" content="/favicon.ico"><meta property="og:image:width" content="2048"><meta property="og:image:height" content="1024"><meta property="article:published_time" content="2019-04-25 00:00:00 +0000 UTC"><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=manifest href=/manifest.json><script type=text/javascript>if('serviceWorker'in navigator){navigator.serviceWorker.register('/sw.js',{scope:'/'}).then(function(registration){console.log('Service Worker Registered');});navigator.serviceWorker.ready.then(function(registration){console.log('Service Worker Ready');});}</script></head><body><div class="container center headings--one-size"><header class=header><div class=header__inner><div class=header__logo><a href=/><div class=logo>Alex Day</div></a></div><div class=menu-trigger>menu</div></div><nav class=menu><ul class="menu__inner menu__inner--desktop"><li><a href=/about>About</a></li><li><a href=/posts>Posts</a></li><li><a href=/projects>Projects</a></li><li><a href=/papers>Papers</a></li><li><a href=/talks>Talks</a></li></ul><ul class="menu__inner menu__inner--mobile"><li><a href=/about>About</a></li><li><a href=/posts>Posts</a></li><li><a href=/projects>Projects</a></li><li><a href=/papers>Papers</a></li><li><a href=/talks>Talks</a></li></ul></nav></header><div class=content><div class=post><h1 class=post-title><a href=/posts/tf-idf/>TF-IDF and Document Summarization</a></h1><div class=post-meta><span class=post-date>2019-04-25</span></div><span class=post-tags>#<a href=/tags/nlp/>NLP</a>&nbsp;</span><div class=post-content><div><p>Term Frequency-Inverse Document Frequency (commonly abbreviated as TF-IDF) is a
formula commonly used in Natural Language Processing (NLP) to determine the
relative importance of a word. The formula is comprised of two sub-formulas,
term frequency and inverse document frequency. The basic assumption of this
formula is that if a word appears more in one document and less in every other
document in the corpus then it is very important to that specific document. In
the following sections I will be describing and giving examples of the two parts
of the formula, explaining how they are combined, and then giving an example of
how to use this to summarize a whole document. This work is based off of the
content in my <a href=/pdf/extractive.pdf>paper on extractive summarization
methods</a>.</p><h2 id=headline-1>Term Frequency</h2><p>The term frequency of a term within a document can be described simply as a raw
count of how many times the term appears within the document, this is shown
below.</p><p>$$ tf(t, d) = f_{t, d} $$</p><p>Where \(t\) is the term to search for and \(d\) is the document to search in.</p><p>This means that if the word <code>cat</code> shows up in your document 10 times then
\(tf(``cat", document) = 10\). There are other ways to calculate the term
frequency, these include binary frequencies (1 if the word is in the document
else 0), frequencies scaled by document length, and others. <a href=https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Definition>Wikipedia</a> has more
information on different formulas for TF.</p><h2 id=headline-2>Inverse Document Frequency</h2><p>This means that if the word <code>cat</code> shows up in your document 10 times then
\(tf(''cat'', document) = 10\). There are other ways to calculate the term
frequency, these include binary frequencies (1 if the word is in the document
else 0), frequencies scaled by document length, and others. <a href=https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Definition>Wikipedia</a> has more
information on different formulas for IDF. The inverse document frequency can be
thought of as an offset that makes sure words such as <code>the</code>, <code>an</code>, <code>and</code>, etc.
do not get large term importance scores. The idf score is lower for a word that
is used throughout the corpus and higher for a word that is used in only one or
two different documents. The equation can be seen below.</p><p>$$ idf(t, D) = log(\frac{|D|}{|\{d \in D : t \in d\}|}) $$</p><p>Where <code>t</code> is the term to search for and <code>D</code> is the set of all documents within the corpus.</p><h2 id=headline-3>Putting it all Together</h2><p>To calculate the TF-IDF of a term the TF and IDF scores of a word are multiplied together.</p><p>$$ tf-idf_{term}(t, d, D) = tf(t, d) \times idf(t, D) $$</p><p>Because TF-IDF cannot be applied to a sentence as a whole we make the assumption
that if a sum score of all the words in a sentence is high then the sentences
relevance is also high. This is seen in the equation below.</p><p>$$ tf-idf_{sentence}(S,d, D) = \frac{1}{|S|} \times \sum_{t \in S}tf-idf_{term}(t, d, D) $$</p><p>Using this you can rank the sentences of a document within a corpus. For
example, when run over the whole Moby Dick corpus the top 3 most important
sentences in chapter one are as follows:</p><ol><li><p>On the contrary, passengers themselves must pay.</p></li><li><p>Whaling voyage by one Ishmael.</p></li><li><p>For to go as a passenger you must needs have a purse, and a purse is but a rag unless you have something in it.</p></li></ol><h2 id=headline-4>Conclusions</h2><p>TF-IDF is a good way to quickly decide on important sentences within a document.
It is used by the <a href=https://www.reddit.com/user/autotldr>autoTLDR</a> bot on Reddit. However, because the algorithm cannot
shorten any sentences nor can it understand the information it will never
produce good sumaries across the board.</p></div></div><div class=pagination><div class=pagination__title><span class=pagination__title-h>Read other posts</span><hr></div><div class=pagination__buttons><span class="button previous"><a href=/posts/naive-emoji-summarization/><span class=button__icon>←</span>
<span class=button__text>Naive Sentence to Emoji Translation</span></a></span></div></div><script src=https://utteranc.es/client.js repo=AlexanderDavid/alexanderdavid.github.io issue-term=pathname theme=photon-dark crossorigin=anonymous async></script></div></div><footer class=footer><div class=footer__inner><div class="copyright copyright--user"><span>© 2020 Alex Day</span>
<span>:: Theme made by <a href=https://twitter.com/panr>panr</a></span></div></div></footer><script src=/assets/main.js></script><script src=/assets/prism.js></script></div></body></html>